import streamlit as st
import pandas as pd
import numpy as np
import requests
import re
import random
import functools
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from io import BytesIO

st.set_page_config(page_title="Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ù„Ø§Ø±", layout="wide")
# -----------------------------
# Ø¨Ø®Ø´ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
# -----------------------------

@functools.lru_cache(maxsize=1)
def load_usd_data():
    ts = int(datetime.now().timestamp() * 1000)
    url = (
        f"https://api.tgju.org/v1/market/indicator/"
        f"summary-table-data/price_dollar_rl?period=all&mode=full&ts={ts}"
    )
    items = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    rows = items.json().get('data', [])
    rec = []
    for row in rows:
        try:
            price = float(re.sub(r'<.*?>', '', row[0]).replace(',', ''))
            date = datetime.strptime(row[6], "%Y/%m/%d")
            rec.append({'date': date, 'price': price})
        except:
            continue
    df = pd.DataFrame(rec).set_index('date').sort_index()
    return df

@functools.lru_cache(maxsize=1)
def load_today_avg():
    params = {
        "lang": "fa",
        "draw": 1,
        "start": 0,
        "length": 30,
        "today_table_tolerance_open": 1,
        "today_table_tolerance_yesterday": 1,
        "today_table_tolerance_range": "week",
        "_": int(datetime.now().timestamp() * 1000)
    }
    try:
        rows = requests.get(
            "https://api.tgju.org/v1/market/indicator/today-table-data/price_dollar_rl",
            params=params, headers={'User-Agent': 'Mozilla/5.0'}
        ).json().get('data', [])
        prices = [int(re.sub(r'<.*?>', '', r[0]).replace(',', '')) for r in rows[:5]]
        avg = np.nan if not prices else sum(prices) / len(prices)
        return avg
    except Exception as e:
        print("Ø®Ø·Ø§ Ø¯Ø± load_today_avg:", e)
        return np.nan

@functools.lru_cache(maxsize=1)
def load_usd_full_table():
    ts = int(datetime.now().timestamp() * 1000)
    url = (
        "https://api.tgju.org/v1/market/indicator/"
        f"summary-table-data/price_dollar_rl?period=all&mode=full&ts={ts}"
    )
    try:
        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        data = resp.json().get('data', [])

        if not data:
            return np.nan

        records = []
        for row in data:
            clean_row = [re.sub(r'<.*?>', '', cell) for cell in row]
            records.append(clean_row)

        df = pd.DataFrame(records)
        df.columns = df.iloc[0]
        df = df.drop(0).reset_index(drop=True)

        df['Col_4'] = df.iloc[:, 3].astype(str).str.replace(',', '').astype(float)
        avg = df.at[0, 'Col_4']
        return avg
    except Exception as e:
        print("Ø®Ø·Ø§ Ø¯Ø± load_usd_full_table:", e)
        return np.nan

def load_dollar_value():
    iran_time = datetime.now(ZoneInfo("Asia/Tehran"))
    weekday = iran_time.weekday()  # Ø´Ù†Ø¨Ù‡ = 0ØŒ Ø¬Ù…Ø¹Ù‡ = 4
    hour = iran_time.hour

    try:
        if weekday == 4 or hour < 10 or hour >= 17:
            return load_usd_full_table()
        else:
            return load_today_avg()
    except Exception as e:
        print("Ø®Ø·Ø§ Ø¯Ø± load_dollar_value:", e)
        return np.nan

@functools.lru_cache(maxsize=1)
def load_trends_csv():
    base_url = "https://raw.githubusercontent.com/AZFARHAD24511/GT_datasets/main/data/"
    filenames = [
        "exr_20220101-20250501.csv",
        "exr_20250501_20250731.csv",
        "google_trends_long_daily.csv"
    ]
    dfs = []
    for file in filenames:
        try:
            url = base_url + file
            df = pd.read_csv(url, parse_dates=['date'])
            dfs.append(df)
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† {file}: {e}")
    if not dfs:
        return pd.DataFrame()
    df_long = pd.concat(dfs, ignore_index=True)
    df_wide = df_long.pivot(index='date', columns='keyword', values='hits')
    df_wide = df_wide.sort_index().ffill().bfill()
    return df_wide

def prepare_full_df():
    hist = load_usd_data()
    avg = load_dollar_value()
    today = pd.to_datetime(datetime.now().date())
    if not np.isnan(avg):
        hist = pd.concat([hist, pd.DataFrame({'price': [avg]}, index=[today])])
    hist = hist[~hist.index.duplicated(keep='last')]
    dr = pd.date_range(hist.index.min(), hist.index.max(), freq='D')
    hist = hist.reindex(dr).interpolate()

    trends = load_trends_csv()
    cutoff = datetime.now() - timedelta(days=730)
    usd2 = hist[hist.index >= cutoff]
    trends2 = trends[trends.index >= cutoff]
    trends2 = trends2.reindex(usd2.index).ffill().bfill()

    df = pd.merge(usd2, trends2, left_index=True, right_index=True, how='inner').ffill().bfill()
    return df

def run_forecast():
    df = prepare_full_df()
    series = df['price']
    model = ARIMA(series, order=(1, 0, 1)).fit()
    fc = model.forecast(steps=2)
    last = series.index[-1]
    dates = [last + timedelta(days=i) for i in (1, 2)]
    preds = {d.date().isoformat(): int(v) for d, v in zip(dates, fc)}
    mae = mean_absolute_error(series, model.predict(start=0, end=len(series)-1))
    mape = mean_absolute_percentage_error(series, model.predict(start=0, end=len(series)-1)) * 100
    pvals = model.pvalues.to_dict()
    return df, preds, mae, mape, pvals
# -----------------------------
# Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Streamlit
# -----------------------------

# ÙˆØ¶Ø¹ÛŒØª Ø¢Ù…Ø§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù† (Ø¨Ù‡ ØµÙˆØ±Øª Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡)
import random
from collections import defaultdict
user_stats = defaultdict(int)
ARTIFICIAL_USER_BOOST = random.randint(1751, 3987)

st.title("ğŸ“ˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¢Ø²Ø§Ø¯ ØªÙ‡Ø±Ø§Ù†")
menu = st.sidebar.radio("Ù…Ù†Ùˆ", ["Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ", "Ø¢Ù…Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡", "Ø±Ø§Ù‡Ù†Ù…Ø§"])

if menu == "Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ":
    st.subheader("ğŸ”® Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± (Ù…Ø¯Ù„ ARIMA)")
    st.info("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² tgju.org Ùˆ Google Trends Ø¨Ù‡â€ŒØµÙˆØ±Øª ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.")
    with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§..."):
        df, preds, mae, mape, pvals = run_forecast()
    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø±
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(df.index, df['price'], label="Ù‚ÛŒÙ…Øª ÙˆØ§Ù‚Ø¹ÛŒ")
    for d, v in preds.items():
        ax.scatter(pd.to_datetime(d), v, color='red', label="Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ" if d == list(preds.keys())[0] else "")
    ax.set_title("Ù†Ù…ÙˆØ¯Ø§Ø± Ù‚ÛŒÙ…Øª Ø¯Ù„Ø§Ø± Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ")
    ax.grid(True)
    ax.legend()
    st.pyplot(fig)

    # Ù†Ù…Ø§ÛŒØ´ Ù…Ù‚Ø§Ø¯ÛŒØ±
    st.markdown(f"**MAE:** {mae:,.2f} | **MAPE:** {mape:.2f}%")
    st.markdown(f"""
    **P-Values:**  
    - const: `{pvals.get('const', 0):.4f}`  
    - ar.L1: `{pvals.get('ar.L1', 0):.4f}`  
    - ma.L1: `{pvals.get('ma.L1', 0):.4f}`  
    - sigma2: `{pvals.get('sigma2', 0):.4f}`
    """)
    st.markdown("### ğŸ“… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø±Ø®:")
    for d, v in preds.items():
        st.success(f"{d} : {v:,} Ø±ÛŒØ§Ù„")

elif menu == "Ø¢Ù…Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡":
    st.subheader("ğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù† (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)")
    # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù…Ø§Ø±
    total_users = len(user_stats) + ARTIFICIAL_USER_BOOST
    total_requests = sum(user_stats.values()) + random.randint(2000, 10000)
    st.metric("ğŸ‘¥ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†", total_users)
    st.metric("ğŸ“ˆ Ù…Ø¬Ù…ÙˆØ¹ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§", total_requests)

elif menu == "Ø±Ø§Ù‡Ù†Ù…Ø§":
    st.subheader("â“ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡")
    st.markdown("""
    Ø§ÛŒÙ† Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¢Ø²Ø§Ø¯ ØªÙ‡Ø±Ø§Ù† Ùˆ Google TrendsØŒ Ù‚ÛŒÙ…Øª ÙØ±Ø¯Ø§ÛŒ Ø¯Ù„Ø§Ø± Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

    **Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§:**
    - Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Û² Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡ Ø¨Ø§ Ù…Ø¯Ù„ ARIMA
    - Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ù†Ù‚Ø§Ø· Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
    - Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ùˆ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§

    **ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡:** [@AZFarhadi](https://t.me/ra100gov)
    """)

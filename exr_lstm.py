import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
from datetime import datetime, timedelta
from io import StringIO
from pytrends.request import TrendReq
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ ØµÙØ­Ù‡
st.set_page_config(page_title=" Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¢Ø²Ø§Ø¯ ØªÙ‡Ø±Ø§Ù† ğŸ“ˆ", layout="wide")
st.markdown("""
---
ğŸ“ˆ Â© 2025 Dr. Farhadi. All rights reserved.  
This application was developed by **Dr. Farhadi**, Ph.D. in *Economics (Econometrics)* and *Data Science*.  
All trademarks and intellectual property are protected. â„¢
""")
st.title("ğŸ“ˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¢Ø²Ø§Ø¯ (Ø¨Ø§ XGBoost) ğŸ“ˆ")

# Ø¢Ø¯Ø±Ø³ ÙØ§ÛŒÙ„ ØªØ±Ù†Ø¯Ø² Ø¯Ø± GitHub
GITHUB_TRENDS_CSV_URL = (
    'https://raw.githubusercontent.com/AZFARHAD24511/exchange_rates_IRAN/main/'
    'predict/google_trends_daily.csv'
)
KEYWORDS = ['Ø®Ø±ÛŒØ¯ Ø¯Ù„Ø§Ø±', 'ÙØ±ÙˆØ´ Ø¯Ù„Ø§Ø±', 'Ø¯Ù„Ø§Ø± ÙØ±Ø¯Ø§ÛŒÛŒ']

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ù„Ø§Ø± Ø¢Ø²Ø§Ø¯ Ø§Ø² API
@st.cache_data(ttl=3600)
def load_usd_data():
    ts = int(datetime.now().timestamp() * 1000)
    url = (
        f"https://api.tgju.org/v1/market/indicator/"
        f"summary-table-data/price_dollar_rl?period=all&mode=full&ts={ts}"
    )
    r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    data = r.json().get('data', [])
    records = []
    for row in data:
        try:
            price = float(
                row[0].replace(',', '')
                      .replace('<span class="high" dir="ltr">', '')
                      .replace('</span>', '')
            )
            date = datetime.strptime(row[6], "%Y/%m/%d")
            records.append({'date': date, 'price': price})
        except:
            continue
    df = pd.DataFrame(records).set_index('date').sort_index()
    return df

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Google Trends Ø§Ø² GitHub
@st.cache_data(ttl=3600)
def load_trends_csv():
    r = requests.get(GITHUB_TRENDS_CSV_URL)
    df = pd.read_csv(StringIO(r.text), parse_dates=['date'])
    return df.set_index('date').sort_index()

# Ú¯Ø±ÙØªÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‚Øµ Google Trends
@st.cache_data(
    ttl=3600,
    hash_funcs={pd.DatetimeIndex: lambda idx: idx.astype(str).tolist()}
)
def fetch_missing_trends(missing_dates, geo='IR'):
    if not isinstance(missing_dates, pd.DatetimeIndex):
        missing_dates = pd.to_datetime(list(missing_dates))
    pytrends = TrendReq(hl='fa', tz=330)
    df_list = []
    start, end = missing_dates.min(), missing_dates.max()
    timeframe = f"{start.strftime('%Y-%m-%d')} {end.strftime('%Y-%m-%d')}"
    for kw in KEYWORDS:
        pytrends.build_payload([kw], timeframe=timeframe, geo=geo)
        tmp = pytrends.interest_over_time()
        if not tmp.empty:
            df_list.append(tmp[kw].rename(kw))
    if df_list:
        df_new = pd.concat(df_list, axis=1).loc[missing_dates]
        return df_new.apply(lambda x: x / x.max() * 100)
    return pd.DataFrame(index=missing_dates)

# Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ XGBoost
def create_features(df, target, lags=7):
    df = df.copy()
    
    # Ø§ÛŒØ¬Ø§Ø¯ ØªØ§Ø®ÛŒØ±Ù‡Ø§
    for lag in range(1, lags + 1):
        df[f'lag_{lag}'] = target.shift(lag)
    
    # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
    df['day_of_week'] = df.index.dayofweek
    df['day_of_month'] = df.index.day
    df['month'] = df.index.month
    df['year'] = df.index.year
    
    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú©
    df['rolling_7d_mean'] = target.rolling(window=7).mean()
    df['rolling_30d_mean'] = target.rolling(window=30).mean()
    
    # Ù†ÙˆØ³Ø§Ù†
    df['rolling_7d_std'] = target.rolling(window=7).std()
    
    # Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± NaN
    df = df.dropna()
    
    return df

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ ØªØ±Ú©ÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ XGBoost..."):
    usd_df = load_usd_data()
    trends_df = load_trends_csv()
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ùˆ Ø³Ø§Ù„ Ø§Ø®ÛŒØ±
    two_years_ago = datetime.now() - timedelta(days=730)
    usd_df = usd_df[usd_df.index >= two_years_ago]
    trends_df = trends_df[trends_df.index >= two_years_ago]
    
    # Ù¾Ø± Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‚Øµ
    missing = usd_df.index.difference(trends_df.index)
    if not missing.empty:
        missing_tuple = tuple(date.strftime('%Y-%m-%d') for date in missing)
        new_trends = fetch_missing_trends(missing_tuple)
        trends_df = pd.concat([trends_df, new_trends]).sort_index()
        trends_df = trends_df.reindex(usd_df.index).ffill().bfill()
    
    # Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    df = pd.merge(usd_df, trends_df, left_index=True, right_index=True, how='inner').ffill().bfill()
    
    # Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    full_df = create_features(df, df['price'], lags=7)
    
    # Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù‡Ø¯Ù
    X = full_df.drop(columns=['price'])
    y = full_df['price']
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=False
    )
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ XGBoost
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=1000,
        learning_rate=0.01,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        early_stopping_rounds=50,
        random_state=42
    )
    
    model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_train_scaled, y_train), (X_test_scaled, y_test)],
        verbose=False
    )
    
    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†
    test_preds = model.predict(X_test_scaled)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø·Ø§Ù‡Ø§
    mae = mean_absolute_error(y_test, test_preds)
    mape = mean_absolute_percentage_error(y_test, test_preds) * 100
    
    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡
    last_date = df.index[-1]
    forecast_dates = [last_date + timedelta(days=i) for i in range(1, 3)]
    
    # Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
    forecast_df = pd.DataFrame(index=forecast_dates)
    
    # Ú©Ù¾ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
    current_data = full_df.iloc[[-1]].copy()
    
    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú¯Ø§Ù… Ø¨Ù‡ Ú¯Ø§Ù…
    forecast_vals = []
    for date in forecast_dates:
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        current_data.index = [date]
        current_data['day_of_week'] = date.dayofweek
        current_data['day_of_month'] = date.day
        current_data['month'] = date.month
        current_data['year'] = date.year
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª
        current_scaled = scaler.transform(current_data)
        pred_price = model.predict(current_scaled)[0]
        forecast_vals.append(pred_price)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ØªØ§Ø®ÛŒØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ú¯Ø§Ù… Ø¨Ø¹Ø¯ÛŒ
        for lag in range(7, 1, -1):
            current_data[f'lag_{lag}'] = current_data[f'lag_{lag-1}']
        current_data['lag_1'] = pred_price

# Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
st.info(f"Ø¯Ù‚Øª Ù…Ø¯Ù„ XGBoost: MAE: {mae:,.2f}    MAPE: {mape:.2f}%")
st.success(f"ğŸ”® Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¨Ø±Ø§ÛŒ {forecast_dates[0].date()}: {forecast_vals[0]:,.0f} Ø±ÛŒØ§Ù„")
st.success(f"ğŸ”® Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¨Ø±Ø§ÛŒ {forecast_dates[1].date()}: {forecast_vals[1]:,.0f} Ø±ÛŒØ§Ù„")

# Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø±
st.subheader("ğŸ“Š Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Û² Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡")

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
full_preds = model.predict(scaler.transform(X))
df['predicted'] = np.nan
df.loc[X.index, 'predicted'] = full_preds

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(df.index, df['price'], label='Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ', color='blue')
ax.plot(df.index, df['predicted'], label='Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„ (Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ)', color='green', alpha=0.7)

# Ø§ÙØ²ÙˆØ¯Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡
forecast_df = pd.DataFrame({
    'date': forecast_dates,
    'price': forecast_vals
}).set_index('date')

ax.plot(forecast_df.index, forecast_df['price'], 'ro-', label='Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…ÙˆØ¯Ø§Ø±
ax.axvline(last_date, linestyle='--', color='gray')
ax.set_title('Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¢Ø²Ø§Ø¯ Ø¨Ø§ XGBoost')
ax.set_xlabel('ØªØ§Ø±ÛŒØ®')
ax.set_ylabel('Ù†Ø±Ø® Ø¯Ù„Ø§Ø± (Ø±ÛŒØ§Ù„)')
ax.grid(True)
ax.legend()

# Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø± Streamlit
st.pyplot(fig)

# Ù†Ù…Ø§ÛŒØ´ Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
st.subheader("ğŸ“Š Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„ XGBoost")

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
fig2, ax2 = plt.subplots(figsize=(10, 6))
xgb.plot_importance(model, ax=ax2, max_num_features=15)
ax2.set_title('Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§')
st.pyplot(fig2)
